# ì˜ìƒ í”¼íŒ… ê´€ë ¨ ì†ŒìŠ¤ì½”ë“œ ìš”ì•½

## ğŸ“‹ ëª©ì°¨
1. [VirtualFittingSystem í´ë˜ìŠ¤ ì „ì²´](#1-virtualfittingsystem-í´ë˜ìŠ¤-ì „ì²´)
2. [app.pyì—ì„œì˜ ì‚¬ìš© ë¶€ë¶„](#2-apppyì—ì„œì˜-ì‚¬ìš©-ë¶€ë¶„)
3. [í•µì‹¬ ë©”ì„œë“œ ì„¤ëª…](#3-í•µì‹¬-ë©”ì„œë“œ-ì„¤ëª…)
4. [ìµœì‹  ê°œì„ ì‚¬í•­](#4-ìµœì‹ -ê°œì„ ì‚¬í•­)

---

## 1. VirtualFittingSystem í´ë˜ìŠ¤ ì „ì²´

**íŒŒì¼ ìœ„ì¹˜**: `src/utils/virtual_fitting.py`

```python
"""
ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ - ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì— ì¶”ì²œ ì½”ë”” í•©ì„±
YOLO íƒì§€ â†’ ì•„ì´í…œë³„ ìƒì„± â†’ ì˜ì—­ í•©ì„± â†’ ìƒ‰ìƒ ë³´ì •
"""

import cv2
import numpy as np
from PIL import Image
import torch
from typing import Dict, List, Tuple, Optional
from diffusers import StableDiffusionInpaintPipeline


class VirtualFittingSystem:
    """ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ - ì‚¬ìš©ì ì´ë¯¸ì§€ì— ì¶”ì²œ ì½”ë”” í•©ì„±"""
    
    def __init__(self, yolo_detector, clip_analyzer):
        """
        Args:
            yolo_detector: YOLODetector ì¸ìŠ¤í„´ìŠ¤
            clip_analyzer: CLIPAnalyzer ì¸ìŠ¤í„´ìŠ¤
        """
        self.yolo_detector = yolo_detector
        self.clip_analyzer = clip_analyzer
        self.inpaint_pipe = None  # inpainting íŒŒì´í”„ë¼ì¸ (í•„ìš” ì‹œ ë¡œë“œ)
        
        # MPS (GPU) ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = "mps"
            self.vae_device = "cpu"
            print("ğŸ MPS (GPU) ì‚¬ìš© ê°€ëŠ¥ - ë¹ ë¥¸ ì´ë¯¸ì§€ ìƒì„±")
        else:
            self.device = "cpu"
            self.vae_device = "cpu"
            print("âš ï¸ MPS ì‚¬ìš© ë¶ˆê°€ - CPU ëª¨ë“œë¡œ ì‹¤í–‰")
    
    def detect_clothing_regions(self, image: Image.Image) -> Dict:
        """
        YOLOë¡œ ì˜ë¥˜ ì˜ì—­ íƒì§€
        
        Returns:
            {
                "top": {"bbox": [x1, y1, x2, y2], "class": "...", "confidence": 0.9},
                "bottom": {"bbox": [...], ...},
                "person": {"bbox": [...], ...}
            }
        """
        # YOLO íƒì§€ ì‹¤í–‰
        result = self.yolo_detector.detect_clothes(image)
        items = result.get("items", [])
        
        regions = {}
        
        # íƒì§€ëœ ì•„ì´í…œì„ ìƒì˜/í•˜ì˜/ì „ì‹ ìœ¼ë¡œ ë¶„ë¥˜
        for item in items:
            class_name = item.get("class", "").lower()
            class_en = item.get("class_en", "").lower()
            bbox = item.get("bbox", [])
            
            if not bbox or len(bbox) != 4:
                continue
            
            # ìƒì˜ ë¶„ë¥˜
            if any(keyword in class_name or keyword in class_en 
                   for keyword in ["ìƒì˜", "top", "shirt", "t-shirt", "jacket", "outwear"]):
                if "top" not in regions or item.get("confidence", 0) > regions["top"].get("confidence", 0):
                    regions["top"] = {
                        "bbox": bbox,
                        "class": item.get("class", ""),
                        "confidence": item.get("confidence", 0)
                    }
            
            # í•˜ì˜ ë¶„ë¥˜
            elif any(keyword in class_name or keyword in class_en 
                     for keyword in ["í•˜ì˜", "bottom", "pants", "ë°”ì§€", "skirt", "ì¹˜ë§ˆ"]):
                if "bottom" not in regions or item.get("confidence", 0) > regions["bottom"].get("confidence", 0):
                    regions["bottom"] = {
                        "bbox": bbox,
                        "class": item.get("class", ""),
                        "confidence": item.get("confidence", 0)
                    }
            
            # ì „ì‹  (person)
            elif "person" in class_name or "person" in class_en:
                if "person" not in regions or item.get("confidence", 0) > regions["person"].get("confidence", 0):
                    regions["person"] = {
                        "bbox": bbox,
                        "class": item.get("class", ""),
                        "confidence": item.get("confidence", 0)
                    }
        
        return regions
    
    def composite_outfit_on_image(self, original_image: Image.Image, 
                                 outfit_items: List[str],
                                 gender: str = "ë‚¨ì„±") -> Optional[Image.Image]:
        """
        ì›ë³¸ ì´ë¯¸ì§€ì— ì¶”ì²œ ì½”ë””ë¥¼ í•©ì„± (í•µì‹¬ ë©”ì„œë“œ)
        
        Args:
            original_image: ì‚¬ìš©ì ì—…ë¡œë“œ ì´ë¯¸ì§€
            outfit_items: ì¶”ì²œ ì½”ë”” ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë¹¨ê°„ìƒ‰ ê¸´íŒ” ì…”ì¸ ", "ê²€ì€ìƒ‰ ë°”ì§€"])
            gender: ì„±ë³„
        
        Returns:
            í•©ì„±ëœ ì´ë¯¸ì§€ ë˜ëŠ” None
        """
        try:
            print("ğŸ¨ ê°€ìƒ í”¼íŒ… ì‹œì‘...")
            print(f"   - ì•„ì´í…œ: {outfit_items}")
            print(f"   - ì„±ë³„: {gender}")
            
            # 1. ì˜ë¥˜ ì˜ì—­ íƒì§€
            regions = self.detect_clothing_regions(original_image)
            
            print(f"   - íƒì§€ëœ ì˜ì—­: {list(regions.keys())}")
            
            if not regions:
                print("âš ï¸ ì˜ë¥˜ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return self._create_text_overlay_image(original_image, outfit_items)
            
            # 2. OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            img_cv = cv2.cvtColor(np.array(original_image), cv2.COLOR_RGB2BGR)
            height, width = img_cv.shape[:2]
            
            # 3. Inpaintingìœ¼ë¡œ ì‹¤ì œ ì˜ë¥˜ í•©ì„±
            self._load_inpaint_pipeline()
            
            if self.inpaint_pipe is None:
                print("âš ï¸ Inpainting ëª¨ë¸ ì—†ìŒ. ê°„ë‹¨í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ì‚¬ìš©")
                return self._simple_color_overlay(img_cv, regions, outfit_items, width, height)
            
            # Inpaintingìœ¼ë¡œ ê° ì•„ì´í…œ í•©ì„± (ìƒì˜ + í•˜ì˜ ëª¨ë‘ ì²˜ë¦¬)
            result_pil = original_image.copy()
            
            # ìƒì˜ì™€ í•˜ì˜ ëª¨ë‘ ì²˜ë¦¬ (ìµœëŒ€ 2ê°œ)
            for idx, item in enumerate(outfit_items[:2]):  # ìƒì˜ + í•˜ì˜
                region_type = "top" if idx == 0 else "bottom"
                
                if region_type not in regions:
                    print(f"âš ï¸ {region_type} ì˜ì—­ ì—†ìŒ, ë‹¤ìŒ ì•„ì´í…œìœ¼ë¡œ")
                    continue
                
                bbox = regions[region_type]["bbox"]
                x1, y1, x2, y2 = [int(v) for v in bbox]
                
                # ë§ˆìŠ¤í¬ ìƒì„± (Inpaintingìš©)
                mask_pil = Image.new("L", (width, height), 0)  # ê²€ì€ìƒ‰
                from PIL import ImageDraw
                draw = ImageDraw.Draw(mask_pil)
                draw.rectangle([x1, y1, x2, y2], fill=255)  # í°ìƒ‰ = êµì²´í•  ì˜ì—­
                
                # í”„ë¡¬í”„íŠ¸ ìƒì„± (region_type ì „ë‹¬!)
                prompt = self._build_inpaint_prompt(item, gender, region_type)
                
                # ì„±ë³„ì— ë”°ë¥¸ negative prompt ê°•í™”
                if gender == "ë‚¨ì„±":
                    negative_prompt = (
                        "woman, female, women's clothing, women's shoes, high heels, "
                        "breasts, cleavage, feminine curves, "
                        "wrong color, mismatched clothes, double clothing, overlay, blur, "
                        "distorted body, unrealistic fabric, old outfit, wrong gender clothing, "
                        "face, head, portrait, drawing, painting, illustration, cartoon, "
                        "anime, unrealistic, fake, artificial, CGI, 3D render, computer graphics"
                    )
                else:  # ì—¬ì„±
                    negative_prompt = (
                        "man, male, men's clothing, men's shoes, "
                        "wrong color, mismatched clothes, double clothing, overlay, blur, "
                        "distorted body, unrealistic fabric, old outfit, wrong gender clothing, "
                        "face, head, portrait, drawing, painting, illustration, cartoon, "
                        "anime, unrealistic, fake, artificial, CGI, 3D render, computer graphics"
                    )
                
                print(f"ğŸ¨ {region_type} ì˜ì—­ Inpainting ì¤‘...")
                print(f"   - í”„ë¡¬í”„íŠ¸: {prompt}")
                
                try:
                    # ì´ë¯¸ì§€ì™€ ë§ˆìŠ¤í¬ë¥¼ ìµœì  í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (í•œ ë²ˆë§Œ, ì†ë„ í–¥ìƒ)
                    # ì›ë³¸ í¬ê¸°ì— ë¹„ë¡€í•˜ì—¬ ë¦¬ì‚¬ì´ì¦ˆ (ë„ˆë¬´ í¬ë©´ ëŠë¦¼)
                    max_size = 512
                    orig_w, orig_h = original_image.size
                    
                    # ë¦¬ì‚¬ì´ì¦ˆ í•„ìš” ì—¬ë¶€ í™•ì¸
                    needs_resize = max(orig_w, orig_h) > max_size
                    
                    if needs_resize:
                        ratio = max_size / max(orig_w, orig_h)
                        target_size = (int(orig_w * ratio), int(orig_h * ratio))
                        # í•œ ë²ˆë§Œ ë¦¬ì‚¬ì´ì¦ˆ
                        result_pil_for_inpaint = result_pil.resize(target_size, Image.Resampling.LANCZOS)
                        mask_pil_for_inpaint = mask_pil.resize(target_size, Image.Resampling.LANCZOS)
                        print(f"   - ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ: {original_image.size} â†’ {target_size}")
                    else:
                        # ë¦¬ì‚¬ì´ì¦ˆ ë¶ˆí•„ìš”
                        result_pil_for_inpaint = result_pil
                        mask_pil_for_inpaint = mask_pil
                        print(f"   - ì›ë³¸ í¬ê¸° ì‚¬ìš©: {original_image.size}")
                    
                    # Inpainting ì‹¤í–‰ (GPU/CPU ëª¨ë“œ, ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„±)
                    with torch.no_grad():
                        try:
                            result = self.inpaint_pipe(
                                prompt=prompt,
                                negative_prompt=negative_prompt,
                                image=result_pil_for_inpaint,
                                mask_image=mask_pil_for_inpaint,
                                num_inference_steps=20 if self.device == "mps" else 10,  # GPU: ë” ë§ì€ steps, CPU: ë¹ ë¥´ê²Œ
                                guidance_scale=9.0,  # í”„ë¡¬í”„íŠ¸ ì¤€ìˆ˜ë„ ë§¤ìš° ë†’ì„
                                strength=0.9  # 90% ë³€ê²½ (ë” ê°•í•˜ê²Œ)
                            )
                        except (RuntimeError, TypeError) as e:
                            error_str = str(e)
                            if "unexpected keyword argument" in error_str and "generator" in error_str:
                                # VAE decode ì‹œê·¸ë‹ˆì²˜ ì˜¤ë¥˜ - íŒ¨ì¹˜ ì¬ì ìš© ë° ì¬ì‹œë„
                                print(f"   âš ï¸ VAE decode ì‹œê·¸ë‹ˆì²˜ ì˜¤ë¥˜, íŒ¨ì¹˜ ì¬ì ìš© ì¤‘...")
                                # VAE decode íŒ¨ì¹˜ ì¬ì ìš©
                                original_decode = self.inpaint_pipe.vae.decode
                                def patched_vae_decode_fix(self_vae, z, return_dict=True, **kwargs):
                                    if z.device.type != "cpu":
                                        z = z.to("cpu", non_blocking=False)
                                    # generator ì¸ì ì œê±°
                                    kwargs.pop('generator', None)
                                    return original_decode(z, return_dict=return_dict, **kwargs)
                                self.inpaint_pipe.vae.decode = patched_vae_decode_fix.__get__(self.inpaint_pipe.vae, type(self.inpaint_pipe.vae))
                                # ì¬ì‹œë„
                                result = self.inpaint_pipe(
                                    prompt=prompt,
                                    negative_prompt=negative_prompt,
                                    image=result_pil_for_inpaint,
                                    mask_image=mask_pil_for_inpaint,
                                    num_inference_steps=20 if self.device == "mps" else 10,
                                    guidance_scale=9.0,
                                    strength=0.9
                                )
                            elif "must be on the same device" in error_str or "same device" in error_str:
                                # ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜ - MPS íŒ¨ì¹˜ ì¬ì ìš©
                                print(f"   âš ï¸ ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜, MPS íŒ¨ì¹˜ ì¬ì ìš© ì¤‘...")
                                # íŒ¨ì¹˜ ì¬ì ìš©
                                self._apply_mps_patches()
                                # ì¬ì‹œë„
                                result = self.inpaint_pipe(
                                    prompt=prompt,
                                    negative_prompt=negative_prompt,
                                    image=result_pil_for_inpaint,
                                    mask_image=mask_pil_for_inpaint,
                                    num_inference_steps=20 if self.device == "mps" else 10,
                                    guidance_scale=9.0,
                                    strength=0.9
                                )
                            else:
                                # ë‹¤ë¥¸ ì˜¤ë¥˜ëŠ” ì¬ë°œìƒ
                                print(f"   âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {error_str[:100]}")
                                raise
                    
                    # ê²°ê³¼ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
                    generated = result.images[0]
                    
                    # ë¦¬ì‚¬ì´ì¦ˆëœ ê²½ìš°ì—ë§Œ ì›ë³¸ í¬ê¸°ë¡œ ë³µì› (í•œ ë²ˆë§Œ)
                    if needs_resize and generated.size != original_image.size:
                        generated = generated.resize(original_image.size, Image.Resampling.LANCZOS)
                        mask_pil_full = mask_pil.resize(original_image.size, Image.Resampling.LANCZOS)
                    else:
                        # ë¦¬ì‚¬ì´ì¦ˆí•˜ì§€ ì•Šì€ ê²½ìš° ë§ˆìŠ¤í¬ë„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                        mask_pil_full = mask_pil
                    
                    # ë§ˆìŠ¤í¬ ì˜ì—­ë§Œ í•©ì„± (ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸ ìœ ì§€)
                    result_np = np.array(result_pil)
                    generated_np = np.array(generated)
                    
                    mask_np = np.array(mask_pil_full) > 127  # ì´ì§„ ë§ˆìŠ¤í¬
                    mask_3d = np.stack([mask_np] * 3, axis=2).astype(float)  # 0.0 ë˜ëŠ” 1.0
                    
                    # ë§ˆìŠ¤í¬ ì˜ì—­ì€ ìƒì„±ëœ ì´ë¯¸ì§€, ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸
                    blended = result_np.astype(float) * (1.0 - mask_3d) + generated_np.astype(float) * mask_3d
                    result_np = np.clip(blended, 0, 255).astype(np.uint8)
                    
                    result_pil = Image.fromarray(result_np)
                    
                    print(f"âœ… {region_type} ì˜ì—­ Inpainting ì™„ë£Œ (ì‹¤ì œ í•©ì„±ë¨)")
                    print(f"   - ë§ˆìŠ¤í¬ ì˜ì—­ í¬ê¸°: {np.sum(mask_np)} í”½ì…€")
                    
                except Exception as e:
                    print(f"âš ï¸ Inpainting ì‹¤íŒ¨: {e}")
                    import traceback
                    traceback.print_exc()
                    return self._simple_color_overlay(img_cv, regions, outfit_items, width, height)
            
            print("âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ (Inpainting)")
            return result_pil
            
        except Exception as e:
            print(f"âš ï¸ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _load_inpaint_pipeline(self):
        """Stable Diffusion Inpainting íŒŒì´í”„ë¼ì¸ ë¡œë“œ"""
        if self.inpaint_pipe is not None:
            return
        
        print("ğŸ¨ Stable Diffusion Inpainting ëª¨ë¸ ë¡œë“œ ì¤‘...")
        print(f"   - ì¥ì¹˜: {self.device.upper()} ëª¨ë“œ")
        
        try:
            self.inpaint_pipe = StableDiffusionInpaintPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-inpainting",
                torch_dtype=torch.float32,
                safety_checker=None,
                device_map=None
            )
            
            # ë””ë°”ì´ìŠ¤ ë°°ì¹˜ (MPS: UNetë§Œ, CPU: VAE/TextEncoder)
            if self.device == "mps":
                self.inpaint_pipe.unet = self.inpaint_pipe.unet.float().to(self.device, non_blocking=False)
                self.inpaint_pipe.vae = self.inpaint_pipe.vae.to(self.vae_device, non_blocking=False)
                self.inpaint_pipe.text_encoder = self.inpaint_pipe.text_encoder.float().to("cpu", non_blocking=False)
                
                # MPS íŒ¨ì¹˜ ì ìš©
                self._patch_vae_for_mps()
                self._apply_mps_patches()
                
                print("âœ… Inpainting ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (MPS/GPU ëª¨ë“œ)")
            else:
                self.inpaint_pipe.unet = self.inpaint_pipe.unet.to("cpu")
                self.inpaint_pipe.vae = self.inpaint_pipe.vae.to("cpu")
                self.inpaint_pipe.text_encoder = self.inpaint_pipe.text_encoder.to("cpu")
                print("âœ… Inpainting ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (CPU ëª¨ë“œ)")
        except Exception as e:
            print(f"âš ï¸ Inpainting ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.inpaint_pipe = None
    
    def _patch_vae_for_mps(self):
        """VAEì˜ encode/decode ë©”ì„œë“œë¥¼ íŒ¨ì¹˜í•˜ì—¬ MPSì™€ í˜¸í™˜ë˜ë„ë¡"""
        if self.device != "mps":
            return
        
        # VAE encode íŒ¨ì¹˜
        original_encode = self.inpaint_pipe.vae.encode
        
        def patched_vae_encode(self_vae, x, return_dict=True, **kwargs):
            if x.device.type != "cpu":
                x = x.to("cpu", non_blocking=False)
            result = original_encode(x, return_dict=return_dict, **kwargs)
            if return_dict:
                if hasattr(result, 'latent_dist'):
                    pass
                return result
            else:
                if isinstance(result, tuple):
                    return tuple(r.to(self.device, non_blocking=False) if isinstance(r, torch.Tensor) and r.device.type != self.device else r for r in result)
                return result.to(self.device, non_blocking=False) if isinstance(result, torch.Tensor) and result.device.type != self.device else result
        
        self.inpaint_pipe.vae.encode = patched_vae_encode.__get__(self.inpaint_pipe.vae, type(self.inpaint_pipe.vae))
        
        # VAE decode íŒ¨ì¹˜ - generator ì¸ì ëª…ì‹œì ìœ¼ë¡œ ì²˜ë¦¬
        original_decode = self.inpaint_pipe.vae.decode
        
        import inspect
        sig = inspect.signature(original_decode)
        print(f"   ğŸ“‹ VAE decode ì›ë³¸ ì‹œê·¸ë‹ˆì²˜: {sig}")
        
        def patched_vae_decode(self_vae, z, return_dict=True, generator=None, **kwargs):
            if z.device.type != "cpu":
                z = z.to("cpu", non_blocking=False)
            kwargs.pop('generator', None)
            return original_decode(z, return_dict=return_dict, **kwargs)
        
        self.inpaint_pipe.vae.decode = patched_vae_decode.__get__(self.inpaint_pipe.vae, type(self.inpaint_pipe.vae))
        
        print("   âœ… VAE encode/decode íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
    
    def _apply_mps_patches(self):
        """MPS ë””ë°”ì´ìŠ¤ ë¶ˆì¼ì¹˜ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•œ íŒ¨ì¹˜ ì ìš©"""
        if self.device != "mps":
            return
        
        # UNet forward íŒ¨ì¹˜
        original_unet_forward = self.inpaint_pipe.unet.forward
        
        def patched_unet_forward(self_unet, sample, timestep, encoder_hidden_states=None, **kwargs):
            if sample.device.type != self.device:
                sample = sample.to(self.device, non_blocking=False)
            if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                timestep = timestep.to(self.device, non_blocking=False)
            if encoder_hidden_states is not None and encoder_hidden_states.device.type != self.device:
                encoder_hidden_states = encoder_hidden_states.to(self.device, non_blocking=False)
            
            for key, value in kwargs.items():
                if isinstance(value, torch.Tensor) and value.device.type != self.device:
                    kwargs[key] = value.to(self.device, non_blocking=False)
            
            return original_unet_forward(sample, timestep, encoder_hidden_states, **kwargs)
        
        self.inpaint_pipe.unet.forward = patched_unet_forward.__get__(self.inpaint_pipe.unet, type(self.inpaint_pipe.unet))
        
        # Scheduler step íŒ¨ì¹˜
        original_scheduler_step = self.inpaint_pipe.scheduler.step
        
        def patched_scheduler_step(self_scheduler, model_output, timestep, sample, **kwargs):
            if model_output.device.type != self.device:
                model_output = model_output.to(self.device, non_blocking=False)
            if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                timestep = timestep.to(self.device, non_blocking=False)
            if sample.device.type != self.device:
                sample = sample.to(self.device, non_blocking=False)
            
            return original_scheduler_step(model_output, timestep, sample, **kwargs)
        
        self.inpaint_pipe.scheduler.step = patched_scheduler_step.__get__(self.inpaint_pipe.scheduler, type(self.inpaint_pipe.scheduler))
        
        # prepare_mask_latents íŒ¨ì¹˜ (ì˜¬ë°”ë¥¸ ì‹œê·¸ë‹ˆì²˜)
        import types
        
        if hasattr(self.inpaint_pipe, 'prepare_mask_latents'):
            original_prepare_mask_latents = self.inpaint_pipe.prepare_mask_latents
            
            def patched_prepare_mask_latents(self_pipe, mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance):
                # deviceë¥¼ MPSë¡œ ê°•ì œ
                device = torch.device(self.device)
                # ì›ë³¸ í˜¸ì¶œ
                mask_latents, masked_image_latents = original_prepare_mask_latents(
                    mask, masked_image, batch_size, height, width, dtype, device, generator, do_classifier_free_guidance
                )
                # ê²°ê³¼ë¥¼ MPSë¡œ ì´ë™
                if mask_latents.device.type != self.device:
                    mask_latents = mask_latents.to(self.device, non_blocking=False)
                if masked_image_latents.device.type != self.device:
                    masked_image_latents = masked_image_latents.to(self.device, non_blocking=False)
                return mask_latents, masked_image_latents
            
            self.inpaint_pipe.prepare_mask_latents = types.MethodType(patched_prepare_mask_latents, self.inpaint_pipe)
        
        # prepare_latents íŒ¨ì¹˜
        if hasattr(self.inpaint_pipe, 'prepare_latents'):
            original_prepare_latents = self.inpaint_pipe.prepare_latents
            
            def patched_prepare_latents(self_pipe, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None, image=None, timestep=None, is_strength_max=True, return_noise=False, return_image_latents=False):
                # deviceë¥¼ MPSë¡œ ê°•ì œ
                device = torch.device(self.device)
                result = original_prepare_latents(
                    batch_size, num_channels_latents, height, width, dtype, device, generator, 
                    latents, image, timestep, is_strength_max, return_noise, return_image_latents
                )
                # ê²°ê³¼ë¥¼ MPSë¡œ ì´ë™
                if isinstance(result, tuple):
                    result = tuple(r.to(self.device, non_blocking=False) if isinstance(r, torch.Tensor) and r.device.type != self.device else r for r in result)
                elif isinstance(result, torch.Tensor) and result.device.type != self.device:
                    result = result.to(self.device, non_blocking=False)
                return result
            
            self.inpaint_pipe.prepare_latents = types.MethodType(patched_prepare_latents, self.inpaint_pipe)
        
        print("   âœ… MPS íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")
    
    def _build_inpaint_prompt(self, item_text: str, gender: str, region_type: str = "top") -> str:
        """
        Inpaintingìš© í”„ë¡¬í”„íŠ¸ ìƒì„± (êµ¬ì²´ì ì´ê³  ì‹œê°ì ì¸ ì§€ì‹œë¬¸)
        
        Args:
            item_text: ì•„ì´í…œ ì„¤ëª… (ì˜ˆ: "ë¹¨ê°„ìƒ‰ ê¸´íŒ” ì…”ì¸ ")
            gender: ì„±ë³„ ("ë‚¨ì„±" ë˜ëŠ” "ì—¬ì„±")
            region_type: "top" ë˜ëŠ” "bottom"
        
        Returns:
            Inpainting í”„ë¡¬í”„íŠ¸
        """
        # ìƒ‰ìƒ/íƒ€ì… ì˜ì–´ ë³€í™˜
        color_map = {
            "ê²€ì€ìƒ‰": "black", "ê²€ì •": "black", "í°ìƒ‰": "white", "í•˜ì–€ìƒ‰": "white",
            "ë¹¨ê°„ìƒ‰": "red", "ë¹¨ê°•": "red", "íŒŒë€ìƒ‰": "blue", "íŒŒë‘": "blue",
            "ë…¸ë€ìƒ‰": "yellow", "ë…¸ë‘": "yellow", "ì´ˆë¡ìƒ‰": "green", "ì´ˆë¡": "green",
            "ë¶„í™ìƒ‰": "pink", "ë¶„í™": "pink", "ë³´ë¼ìƒ‰": "purple", "ë³´ë¼": "purple",
            "íšŒìƒ‰": "gray", "íšŒìƒ‰í†¤": "gray", "ê°ˆìƒ‰": "brown", "ë² ì´ì§€": "beige",
            "ì¹´í‚¤": "khaki", "ë„¤ì´ë¹„": "navy", "ì˜¤ë Œì§€": "orange", "íŒŒìŠ¤í…”": "pastel"
        }
        
        # ì˜ë¥˜ íƒ€ì… ë° ì¬ì§ˆ ë³€í™˜
        item_map = {
            "ë°˜íŒ”": "short sleeve", "ê¸´íŒ”": "long sleeve",
            "í‹°ì…”ì¸ ": "t-shirt", "í‹°": "t-shirt", "ì…”ì¸ ": "shirt",
            "ë°”ì§€": "pants", "íŒ¬ì¸ ": "pants", "ë°˜ë°”ì§€": "shorts",
            "ì¬í‚·": "jacket", "ìì¼“": "jacket", "ê°€ë””ê±´": "cardigan",
            "ì½”íŠ¸": "coat", "íŠ¸ë Œì¹˜ì½”íŠ¸": "trench coat",
            "ì²­ë°”ì§€": "jeans", "ì§„": "jeans",
            "ìŠ¤ë‹ˆì»¤ì¦ˆ": "sneakers", "ìŠ¤ë‹ˆì»¤": "sneakers",
            "ë¶€ì¸ ": "boots", "ì‹ ë°œ": "shoes",
            "ì„ ê¸€ë¼ìŠ¤": "sunglasses", "ì•ˆê²½": "glasses",
            "ë¦°ë„¨": "linen", "ë©´": "cotton", "ìš¸": "wool",
            "ë‹ˆíŠ¸": "knit", "ìŠ¤ì›¨í„°": "sweater"
        }
        
        # ì¬ì§ˆ ì¶”ì¶œ
        fabric_map = {
            "ë©´": "cotton", "ë¦°ë„¨": "linen", "ìš¸": "wool", "ë‹ˆíŠ¸": "knit",
            "ë°ë‹˜": "denim", "ì²­": "denim", "ê°€ì£½": "leather", "ì‹¤í¬": "silk"
        }
        
        # ë³€í™˜
        en_item = item_text
        
        # ìƒ‰ìƒ ì¶”ì¶œ
        extracted_colors = []
        for kr, en in color_map.items():
            if kr in item_text:
                extracted_colors.append(en)
                en_item = en_item.replace(kr, en)
        
        extracted_color = extracted_colors[0] if extracted_colors else None
        
        # ì˜ë¥˜ íƒ€ì… ì¶”ì¶œ
        extracted_type = None
        for kr, en in item_map.items():
            if kr in item_text:
                extracted_type = en
                en_item = en_item.replace(kr, en)
        
        # ì¬ì§ˆ ì¶”ì¶œ
        extracted_fabric = None
        for kr, en in fabric_map.items():
            if kr in item_text:
                extracted_fabric = en
                break
        
        # ë‚¨ì€ í•œê¸€ ë‹¨ì–´ ì œê±°
        import re
        en_item = re.sub(r'[ê°€-í£]+', '', en_item).strip()
        en_item = re.sub(r'\s+', ' ', en_item).strip()
        en_item = re.sub(r'\s*(ë˜ëŠ”|or)\s*.*', '', en_item, flags=re.IGNORECASE).strip()
        
        # ì„±ë³„ ëª…í™•íˆ ì§€ì •
        gender_kw = "man" if gender == "ë‚¨ì„±" else "woman" if gender == "ì—¬ì„±" else "person"
        
        # êµ¬ì²´ì ì´ê³  ì‹œê°ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (ìƒ‰ìƒê³¼ íƒ€ì… ì •í™•íˆ ëª…ì‹œ)
        if region_type == "top":
            # ìƒì˜
            if extracted_type and extracted_color:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                # íƒ€ì… ì •í™•íˆ ì§€ì •
                if "long sleeve" in extracted_type or "ê¸´íŒ”" in item_text:
                    type_spec = "long sleeve shirt"
                elif "short sleeve" in extracted_type or "ë°˜íŒ”" in item_text:
                    type_spec = "short sleeve t-shirt"
                else:
                    type_spec = "shirt"
                
                prompt = (
                    f"a {gender_kw} wearing a {extracted_color} {type_spec}, "
                    f"EXACTLY {extracted_color} color, {fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            elif extracted_type:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                if "long sleeve" in extracted_type or "ê¸´íŒ”" in item_text:
                    type_spec = "long sleeve shirt"
                elif "short sleeve" in extracted_type or "ë°˜íŒ”" in item_text:
                    type_spec = "short sleeve t-shirt"
                else:
                    type_spec = "shirt"
                
                prompt = (
                    f"a {gender_kw} wearing {type_spec}, "
                    f"{fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            else:
                prompt = (
                    f"a {gender_kw} wearing upper body clothing, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
        else:
            # í•˜ì˜
            if extracted_type and extracted_color:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                # íƒ€ì… ì •í™•íˆ ì§€ì •
                if "pants" in extracted_type or "ë°”ì§€" in item_text:
                    type_spec = "slim-fit trousers"
                elif "shorts" in extracted_type or "ë°˜ë°”ì§€" in item_text:
                    type_spec = "shorts"
                else:
                    type_spec = "pants"
                
                prompt = (
                    f"a {gender_kw} wearing {extracted_color} {type_spec}, "
                    f"EXACTLY {extracted_color} color, {fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            elif extracted_type:
                fabric_part = f"{extracted_fabric} fabric" if extracted_fabric else "cotton fabric"
                if "pants" in extracted_type or "ë°”ì§€" in item_text:
                    type_spec = "slim-fit trousers"
                elif "shorts" in extracted_type or "ë°˜ë°”ì§€" in item_text:
                    type_spec = "shorts"
                else:
                    type_spec = "pants"
                
                prompt = (
                    f"a {gender_kw} wearing {type_spec}, "
                    f"{fabric_part}, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
            else:
                prompt = (
                    f"a {gender_kw} wearing lower body clothing, "
                    f"realistic fit, naturally worn, proper draping, natural folds, "
                    f"realistic lighting, natural shadows, high quality photo, "
                    f"professional photography, authentic clothing texture"
                )
        
        return prompt
    
    def _simple_color_overlay(self, img_cv: np.ndarray, regions: Dict, 
                             outfit_items: List[str], width: int, height: int) -> Image.Image:
        """í´ë°±: ê°„ë‹¨í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ (Inpainting ì‹¤íŒ¨ ì‹œ)"""
        result_img = img_cv.copy()
        
        for idx, item in enumerate(outfit_items[:2]):
            region_type = "top" if idx == 0 else "bottom"
            
            if region_type not in regions:
                continue
            
            bbox = regions[region_type]["bbox"]
            x1, y1, x2, y2 = [int(v) for v in bbox]
            
            color_bgr = self._extract_target_color(item)
            
            if color_bgr is not None:
                roi = result_img[y1:y2, x1:x2].copy()
                roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)
                colored_roi = np.full_like(roi, color_bgr, dtype=np.uint8)
                
                for c in range(3):
                    colored_roi[:, :, c] = np.clip(
                        colored_roi[:, :, c] * (roi_gray.astype(float) / 128.0),
                        0, 255
                    ).astype(np.uint8)
                
                alpha = 0.8
                blended_roi = cv2.addWeighted(colored_roi, alpha, roi, 1-alpha, 0)
                result_img[y1:y2, x1:x2] = blended_roi
                
                print(f"âœ… {region_type} ì˜ì—­ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ì ìš©")
        
        return Image.fromarray(cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB))
    
    def _extract_target_color(self, item_text: str) -> Optional[Tuple[int, int, int]]:
        """ì•„ì´í…œ í…ìŠ¤íŠ¸ì—ì„œ ëª©í‘œ ìƒ‰ìƒ ì¶”ì¶œ (BGR)"""
        color_map_bgr = {
            "ê²€ì€ìƒ‰": (0, 0, 0),
            "í°ìƒ‰": (255, 255, 255),
            "ë¹¨ê°„ìƒ‰": (0, 0, 255),
            "íŒŒë€ìƒ‰": (255, 0, 0),
            "ë…¸ë€ìƒ‰": (0, 255, 255),
            "ì´ˆë¡ìƒ‰": (0, 255, 0),
            "íšŒìƒ‰": (128, 128, 128),
            "ê°ˆìƒ‰": (42, 42, 165),
            "ë² ì´ì§€": (220, 245, 245),
            "ë„¤ì´ë¹„": (128, 0, 0),
            "ë¶„í™ìƒ‰": (203, 192, 255),
        }
        
        for color_name, bgr in color_map_bgr.items():
            if color_name in item_text:
                return bgr
        
        return None
    
    def _create_text_overlay_image(self, image: Image.Image, items: List[str]) -> Image.Image:
        """ì˜ë¥˜ íƒì§€ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ì— í…ìŠ¤íŠ¸ ì˜¤ë²„ë ˆì´"""
        from PIL import ImageDraw
        
        img_with_text = image.copy()
        draw = ImageDraw.Draw(img_with_text)
        
        text_lines = ["ì¶”ì²œ ì½”ë””:"] + items
        y_offset = 20
        
        for line in text_lines:
            text_bbox = draw.textbbox((10, y_offset), line)
            draw.rectangle(
                [(text_bbox[0]-5, text_bbox[1]-5), (text_bbox[2]+5, text_bbox[3]+5)], 
                fill=(255, 255, 255)
            )
            draw.text((10, y_offset), line, fill=(0, 0, 0))
            y_offset += 25
        
        return img_with_text
```

---

## 2. app.pyì—ì„œì˜ ì‚¬ìš© ë¶€ë¶„

**íŒŒì¼ ìœ„ì¹˜**: `app.py`

### 2.1 ì´ˆê¸°í™” ë¶€ë¶„

```python
from src.utils.virtual_fitting import VirtualFittingSystem

# ì„¸ì…˜ ìƒíƒœì— ê°€ìƒ í”¼íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
if 'virtual_fitting' not in st.session_state:
    st.session_state.virtual_fitting = VirtualFittingSystem(
        st.session_state.fashion_recommender.detector,
        st.session_state.fashion_recommender.analyzer
    )
```

### 2.2 í†µí•© ì¶”ì²œ ìƒì„± ë¶€ë¶„

```python
# í†µí•© ì¶”ì²œ ìƒì„± (ì„±ë³„ + MBTI + ì´ë¯¸ì§€ ë¶„ì„ + ì˜¨ë„/ê³„ì ˆ â†’ ìŠ¤íƒ€ì¼ â†’ ì•„ì´í…œ â†’ ì œí’ˆ)
unified_recommendations = st.session_state.recommendation_engine.generate_unified_outfit_recommendations(
    gender, mbti, temp, weather, season,
    detected_items=detected_items_data.get("items", []),
    style_analysis=style_analysis_data
)

# ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€ìš©
recommendations = st.session_state.recommendation_engine.get_personalized_recommendation(
    mbti, temp, weather, season,
    detected_items=detected_items_data.get("items", []),
    style_analysis=style_analysis_data
)

# í†µí•© ì¶”ì²œ ê²°ê³¼ë¥¼ ê¸°ì¡´ recommendationsì— ë³‘í•©
recommendations["outfit_versions"] = unified_recommendations["outfit_versions"]
```

### 2.3 ì¶”ì²œ ì½”ë”” í‘œì‹œ ë¶€ë¶„

```python
# í†µí•© ì¶”ì²œ ê²°ê³¼ ì‚¬ìš©
outfit_versions = recommendations.get("outfit_versions", [])

if outfit_versions and len(outfit_versions) >= 3:
    # í†µí•© ì¶”ì²œ ì‚¬ìš© (ì„±ë³„ + MBTI + ì´ë¯¸ì§€ ë¶„ì„ + ì˜¨ë„/ê³„ì ˆ)
    for idx, (col, version) in enumerate(zip([col1, col2, col3], outfit_versions[:3])):
        with col:
            st.write(f"**ì¶”ì²œ ì½”ë”” {idx+1}**")
            st.write(f"**{version['style']}**")
            
            st.info(version['description'])
            st.write(f"**ì•„ì´í…œ:**")
            
            # ì•„ì´í…œ í‘œì‹œ
            for item in version['items']:
                st.write(f"â€¢ {item}")
            
            # ì¶”ì²œ ì œí’ˆ í‘œì‹œ
            st.write("**ì¶”ì²œ ì œí’ˆ:**")
            for product in version['products']:
                st.write(f"â€¢ {product}")
            
            # ê°€ìƒ í”¼íŒ…/AI ìƒì„±ìš© ë°ì´í„° ì €ì¥
            outfit_desc = {
                "items": version['items'],
                "style": version['style'],
                "colors": [item.split()[0] for item in version['items'] if item.split()[0] in ["ê²€ì€ìƒ‰", "í°ìƒ‰", "ë¹¨ê°„ìƒ‰", "íŒŒë€ìƒ‰", "íšŒìƒ‰", "ë² ì´ì§€", "ë„¤ì´ë¹„"]][:2],
                "gender": version['gender']
            }
            current_image_hash = st.session_state.get("last_image_hash", "default")
            cache_key = f"generated_image_{current_image_hash}_{version['style']}_{idx}"
            outfit_data_list.append({
                "col": col,
                "outfit_desc": outfit_desc,
                "style": version['style'],
                "idx": idx,
                "cache_key": cache_key
            })
```

### 2.4 ê°€ìƒ í”¼íŒ… ì‹¤í–‰ ë¶€ë¶„

```python
# ê°€ìƒ í”¼íŒ… ëª¨ë“œ ì„ íƒ
fitting_mode = st.radio(
    "ì´ë¯¸ì§€ ìƒì„± ë°©ì‹",
    ["ê°€ìƒ í”¼íŒ… (ì¶”ì²œ)", "AI ìƒì„± (ì‹¤í—˜ì )"],
    index=0,
    key="fitting_mode"
)

# ì¶”ì²œ ì½”ë”” í‘œì‹œ
if fitting_mode == "ê°€ìƒ í”¼íŒ… (ì¶”ì²œ)":
    for data in outfit_data_list:
        with data["col"]:
            # ìºì‹œ í™•ì¸
            cache_key = f"virtual_fitting_{data['cache_key']}"
            
            if cache_key not in st.session_state:
                with st.spinner(f"ğŸ¨ {data['style']} ìŠ¤íƒ€ì¼ ê°€ìƒ í”¼íŒ… ì¤‘..."):
                    # ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    source_image = user_uploaded_image if user_uploaded_image is not None else image
                    
                    # ê°€ìƒ í”¼íŒ… ì‹¤í–‰
                    fitted_image = st.session_state.virtual_fitting.composite_outfit_on_image(
                        source_image,
                        data["outfit_desc"]["items"],
                        data["outfit_desc"]["gender"]
                    )
                    
                    if fitted_image:
                        st.session_state[cache_key] = fitted_image
                        st.image(fitted_image, caption=f"{data['style']} ìŠ¤íƒ€ì¼ ê°€ìƒ í”¼íŒ…", width='stretch')
                        st.success("âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ")
                    else:
                        st.warning("âš ï¸ ê°€ìƒ í”¼íŒ… ì‹¤íŒ¨ - ì˜ë¥˜ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            else:
                # ìºì‹œëœ ì´ë¯¸ì§€ ì‚¬ìš©
                cached_image = st.session_state[cache_key]
                st.image(cached_image, caption=f"{data['style']} ìŠ¤íƒ€ì¼ ê°€ìƒ í”¼íŒ…", width='stretch')
                st.success("âœ… ê°€ìƒ í”¼íŒ… ì™„ë£Œ (ìºì‹œ)")
```

---

## 3. í•µì‹¬ ë©”ì„œë“œ ì„¤ëª…

### 3.1 `composite_outfit_on_image()` - ë©”ì¸ í•©ì„± ë©”ì„œë“œ
- **ì…ë ¥**: ì›ë³¸ ì´ë¯¸ì§€, ì¶”ì²œ ì•„ì´í…œ ë¦¬ìŠ¤íŠ¸, ì„±ë³„
- **ì²˜ë¦¬ ê³¼ì •**:
  1. YOLOë¡œ ì˜ë¥˜ ì˜ì—­ íƒì§€ (ìƒì˜/í•˜ì˜)
  2. Stable Diffusion Inpaintingìœ¼ë¡œ ê° ì˜ì—­ì— ì˜ìƒ ìƒì„±
  3. ìƒì„±ëœ ì´ë¯¸ì§€ë¥¼ ì›ë³¸ê³¼ ë¸”ë Œë”©
- **ì¶œë ¥**: í•©ì„±ëœ ì´ë¯¸ì§€
- **ê°œì„ ì‚¬í•­**:
  - ë¦¬ì‚¬ì´ì¦ˆ ìµœì í™”: `needs_resize` í”Œë˜ê·¸ë¡œ ë¶ˆí•„ìš”í•œ ë¦¬ì‚¬ì´ì¦ˆ ë°©ì§€
  - ì„±ë³„ ê¸°ë°˜ negative prompt: ë‚¨ì„±/ì—¬ì„±ì— ë§ëŠ” í‚¤ì›Œë“œ ì œê±°
  - ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”: VAE decode ì‹œê·¸ë‹ˆì²˜ ì˜¤ë¥˜, ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜ ìë™ ì¬ì‹œë„

### 3.2 `detect_clothing_regions()` - ì˜ë¥˜ ì˜ì—­ íƒì§€
- YOLO íƒì§€ ê²°ê³¼ë¥¼ ìƒì˜/í•˜ì˜/ì „ì‹ ìœ¼ë¡œ ë¶„ë¥˜
- ê°€ì¥ ë†’ì€ confidenceì˜ ê²°ê³¼ë§Œ ì„ íƒ

### 3.3 `_load_inpaint_pipeline()` - ëª¨ë¸ ë¡œë“œ
- Stable Diffusion 2 Inpainting ëª¨ë¸ ë¡œë“œ
- MPS(GPU) ëª¨ë“œ ìë™ ê°ì§€ ë° íŒ¨ì¹˜ ì ìš©
- VAEëŠ” CPU, UNetì€ MPSë¡œ ë°°ì¹˜

### 3.4 `_apply_mps_patches()` - MPS í˜¸í™˜ì„± íŒ¨ì¹˜
- UNet forward íŒ¨ì¹˜: ëª¨ë“  í…ì„œë¥¼ MPSë¡œ ì´ë™
- Scheduler step íŒ¨ì¹˜: í…ì„œ ë””ë°”ì´ìŠ¤ ì¼ì¹˜
- prepare_mask_latents íŒ¨ì¹˜: ë§ˆìŠ¤í¬ë¥¼ MPSë¡œ ì´ë™ (ì˜¬ë°”ë¥¸ ì‹œê·¸ë‹ˆì²˜)
- prepare_latents íŒ¨ì¹˜: latentë¥¼ MPSë¡œ ì´ë™

### 3.5 `_build_inpaint_prompt()` - í”„ë¡¬í”„íŠ¸ ìƒì„± (ì¤‘ìš” ê°œì„ )
- **í•œê¸€ ì•„ì´í…œ í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜**
- **ìƒ‰ìƒ ì •í™•ë„ í–¥ìƒ**: `EXACTLY {color} color` ëª…ì‹œ
- **íƒ€ì… êµ¬ì²´í™”**:
  - ìƒì˜: `long sleeve shirt`, `short sleeve t-shirt`, `shirt`
  - í•˜ì˜: `slim-fit trousers`, `shorts`, `pants`
- **ì¬ì§ˆ ì¶”ì¶œ**: `fabric_map`ì„ í†µí•´ ë©´, ë¦°ë„¨, ìš¸ ë“± ì¬ì§ˆ ì •ë³´ í¬í•¨
- **ì„±ë³„ ëª…ì‹œ**: `a man wearing...` ë˜ëŠ” `a woman wearing...` ëª…í™•íˆ ì§€ì •
- **ìì—°ìŠ¤ëŸ¬ìš´ ì°©ìš©ê°**: `realistic fit, naturally worn, proper draping, natural folds` í‚¤ì›Œë“œ ì¶”ê°€
- **ê³ í’ˆì§ˆ í‘œí˜„**: `high quality photo, professional photography, authentic clothing texture` ì¶”ê°€

### 3.6 `_simple_color_overlay()` - í´ë°± ë©”ì„œë“œ
- Inpainting ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ ì ìš©
- ë°”ìš´ë”©ë°•ìŠ¤ ì˜ì—­ì— ìƒ‰ìƒë§Œ ë³€ê²½

### 3.7 `_patch_vae_for_mps()` - VAE íŒ¨ì¹˜
- VAE encode: ì…ë ¥ì„ CPUë¡œ ì´ë™, ê²°ê³¼ë¥¼ MPSë¡œ ì´ë™
- VAE decode: `generator` ì¸ì ì œê±° ë° CPU ì²˜ë¦¬

---

## 4. ìµœì‹  ê°œì„ ì‚¬í•­

### 4.1 í”„ë¡¬í”„íŠ¸ ì •í™•ë„ í–¥ìƒ
- **ìƒ‰ìƒ ëª…ì‹œ**: `EXACTLY {color} color` ì¶”ê°€ë¡œ ìƒ‰ìƒ ì •í™•ë„ í–¥ìƒ
- **íƒ€ì… êµ¬ì²´í™”**: ê¸´íŒ”/ë°˜íŒ”, ë°”ì§€/ë°˜ë°”ì§€ êµ¬ë¶„
- **ì¬ì§ˆ ì •ë³´**: ë©´, ë¦°ë„¨, ìš¸ ë“± ì¬ì§ˆ ì •ë³´ í¬í•¨
- **ì„±ë³„ ëª…ì‹œ**: í”„ë¡¬í”„íŠ¸ì— `a man` ë˜ëŠ” `a woman` ëª…ì‹œ

### 4.2 Negative Prompt ê°•í™”
- **ì„±ë³„ ê¸°ë°˜ ì œê±°**: ë‚¨ì„±ì¼ ê²½ìš° ì—¬ì„± ê´€ë ¨ í‚¤ì›Œë“œ ì œê±°, ì—¬ì„±ì¼ ê²½ìš° ë‚¨ì„± ê´€ë ¨ í‚¤ì›Œë“œ ì œê±°
- **ì˜ˆì‹œ**:
  - ë‚¨ì„±: `"woman, female, women's clothing, women's shoes, high heels, breasts, cleavage, feminine curves"`
  - ì—¬ì„±: `"man, male, men's clothing, men's shoes"`

### 4.3 ë¦¬ì‚¬ì´ì¦ˆ ìµœì í™”
- **ë¶ˆí•„ìš”í•œ ë¦¬ì‚¬ì´ì¦ˆ ë°©ì§€**: `needs_resize` í”Œë˜ê·¸ë¡œ ì›ë³¸ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë¦¬ì‚¬ì´ì¦ˆ ìƒëµ
- **í•œ ë²ˆë§Œ ë¦¬ì‚¬ì´ì¦ˆ**: ë¦¬ì‚¬ì´ì¦ˆëœ ê²½ìš°ì—ë§Œ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›í•˜ì—¬ ì´ì¤‘ ë¦¬ì‚¬ì´ì¦ˆ ë°©ì§€

### 4.4 í†µí•© ì¶”ì²œ ë¡œì§ ì—°ë™
- **ì„±ë³„ + MBTI + ì´ë¯¸ì§€ ë¶„ì„ + ì˜¨ë„/ê³„ì ˆ**: ëª¨ë“  ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ìŠ¤íƒ€ì¼ ìƒì„±
- **ìŠ¤íƒ€ì¼ â†’ ì•„ì´í…œ â†’ ì œí’ˆ**: ì¼ê´€ëœ ìˆœì„œë¡œ ì¶”ì²œ ìƒì„±
- **3ê°€ì§€ ë²„ì „ í†µì¼**: ëª¨ë“  ì¶”ì²œ ì½”ë””ê°€ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ í‘œì‹œ

### 4.5 ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
- **VAE decode ì‹œê·¸ë‹ˆì²˜ ì˜¤ë¥˜**: ìë™ ê°ì§€ ë° ì¬íŒ¨ì¹˜
- **ë””ë°”ì´ìŠ¤ ì˜¤ë¥˜**: MPS íŒ¨ì¹˜ ì¬ì ìš© ë° ì¬ì‹œë„
- **í´ë°± ë©”ì»¤ë‹ˆì¦˜**: ì—ëŸ¬ ë°œìƒ ì‹œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ë¡œ ìë™ ì „í™˜

---

## ğŸ“¦ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

```python
pip install opencv-python pillow numpy torch diffusers accelerate
```

---

## ğŸ”§ ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒ

1. **YOLOv5**: ì˜ë¥˜ ì˜ì—­ íƒì§€
2. **Stable Diffusion 2 Inpainting**: ì˜ìƒ ìƒì„±
3. **MPS (Metal Performance Shaders)**: Apple Silicon GPU ê°€ì†
4. **OpenCV**: ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ë¸”ë Œë”©
5. **PIL/Pillow**: ì´ë¯¸ì§€ ì¡°ì‘

---

## ğŸ“ ì°¸ê³ ì‚¬í•­

- MPS ëª¨ë“œì—ì„œ VAEëŠ” CPUì—ì„œ ì‹¤í–‰ (ì•ˆì •ì„±)
- UNetì€ MPSì—ì„œ ì‹¤í–‰ (ì†ë„)
- ì´ë¯¸ì§€ëŠ” 512pxë¡œ ë¦¬ì‚¬ì´ì¦ˆ í›„ ì²˜ë¦¬ (ì†ë„ í–¥ìƒ, ë¶ˆí•„ìš” ì‹œ ìƒëµ)
- ë§ˆìŠ¤í¬ ê¸°ë°˜ ë¸”ë Œë”©ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ í•©ì„±
- ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ìœ¼ë¡œ ìƒ‰ìƒ ì˜¤ë²„ë ˆì´ë¡œ í´ë°±
- í”„ë¡¬í”„íŠ¸ì— `EXACTLY {color} color` ëª…ì‹œë¡œ ìƒ‰ìƒ ì •í™•ë„ í–¥ìƒ
- ì„±ë³„ ê¸°ë°˜ negative promptë¡œ ì„±ë³„ ë¶ˆì¼ì¹˜ ë°©ì§€
- íƒ€ì… êµ¬ì²´í™” (long sleeve shirt, slim-fit trousers)ë¡œ ì˜ë¥˜ ì •í™•ë„ í–¥ìƒ

---

## ğŸ¯ ìµœì‹  í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ

### ìƒì˜ ì˜ˆì‹œ
```
a man wearing a black long sleeve shirt, EXACTLY black color, cotton fabric, 
realistic fit, naturally worn, proper draping, natural folds, 
realistic lighting, natural shadows, high quality photo, 
professional photography, authentic clothing texture
```

### í•˜ì˜ ì˜ˆì‹œ
```
a man wearing gray slim-fit trousers, EXACTLY gray color, cotton fabric, 
realistic fit, naturally worn, proper draping, natural folds, 
realistic lighting, natural shadows, high quality photo, 
professional photography, authentic clothing texture
```

### Negative Prompt (ë‚¨ì„±)
```
woman, female, women's clothing, women's shoes, high heels, 
breasts, cleavage, feminine curves, 
wrong color, mismatched clothes, double clothing, overlay, blur, 
distorted body, unrealistic fabric, old outfit, wrong gender clothing, 
face, head, portrait, drawing, painting, illustration, cartoon, 
anime, unrealistic, fake, artificial, CGI, 3D render, computer graphics
```
