"""
ì¶”ì²œ ì½”ë”” AI ì´ë¯¸ì§€ ìƒì„± ìœ í‹¸ë¦¬í‹°
Stable Diffusion ë¡œì»¬ ì‹¤í–‰ (MPS ìµœì í™”)
"""

import os
from PIL import Image
from typing import Optional, Dict
import torch
from diffusers import StableDiffusionPipeline


class OutfitImageGenerator:
    """ì¶”ì²œ ì½”ë”” AI ì´ë¯¸ì§€ ìƒì„± í´ë˜ìŠ¤ - ê°„ì†Œí™” ë²„ì „"""
    
    def __init__(self, method: str = "stable_diffusion"):
        """
        Args:
            method: ì´ë¯¸ì§€ ìƒì„± ë°©ë²• (í˜„ì¬ëŠ” "stable_diffusion"ë§Œ ì§€ì›)
        """
        self.method = method
        
        # MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
            raise RuntimeError("âŒ MPS (GPU)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. MacBook M1/M2ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        # íŒŒì´í”„ë¼ì¸ì€ ë‚˜ì¤‘ì— ë¡œë“œ (ì§€ì—° ë¡œë”©)
        self.pipe = None
        self.device = "mps"
        self.vae_device = "cpu"
    
    def _load_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì§€ì—° ë¡œë”©"""
        if self.pipe is not None:
            return
        
        print("ğŸ Apple Silicon (M1/M2) ê°ì§€ - MPS ë°±ì—”ë“œ ì‚¬ìš© (GPU ê°€ì†)")
        print("âš¡ Stable Diffusion 2.1 ëª¨ë¸ ë¡œë“œ ì¤‘... (SD 1.4ë³´ë‹¤ ê°œì„ ëœ ë²„ì „)")
        print("â³ ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤ (ì•½ 5GB, ëª‡ ë¶„ ì†Œìš”)")
        
        try:
            # Stable Diffusion 2.1: SD 1.4ë³´ë‹¤ ê°œì„ , ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
            print("ğŸ“¦ Stable Diffusion 2.1 ì‹œë„ ì¤‘... (SD 1.4ë³´ë‹¤ ê°œì„ )")
            self.pipe = StableDiffusionPipeline.from_pretrained(
                "stabilityai/stable-diffusion-2-1",
                torch_dtype=torch.float32,
                safety_checker=None,
                requires_safety_checker=False,
                device_map=None
            )
            print("âœ… Stable Diffusion 2.1 ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            print(f"âš ï¸ SD 2.1 ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ“¦ Stable Diffusion v1.4ë¡œ í´ë°±...")
            
            # í´ë°±: SD 1.4
            self.pipe = StableDiffusionPipeline.from_pretrained(
                "CompVis/stable-diffusion-v1-4",
                torch_dtype=torch.float32,
                safety_checker=None,
                requires_safety_checker=False,
                device_map=None
            )
            print("âœ… SD 1.4 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ì»´í¬ë„ŒíŠ¸ë³„ ì¥ì¹˜ ë°°ì¹˜
        self.pipe.vae = self.pipe.vae.to(self.vae_device, non_blocking=False)
        self.pipe.unet = self.pipe.unet.float().to(self.device, non_blocking=False)
        self.pipe.text_encoder = self.pipe.text_encoder.float().to("cpu", non_blocking=False)
        
        # ìµœì í™” ì„¤ì •
        self.pipe.enable_attention_slicing()
        try:
            self.pipe.enable_xformers_memory_efficient_attention()
        except:
            pass
        
        torch.mps.synchronize()
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _build_prompt(self, outfit_description: Dict) -> str:
        """íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„± (ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ)"""
        items = outfit_description.get("items", [])
        style = outfit_description.get("style", "ìºì£¼ì–¼")
        gender = outfit_description.get("gender", "ê³µìš©")
        
        # ì„±ë³„ í‚¤ì›Œë“œ (ê°„ê²°)
        gender_keyword = "man" if gender == "ë‚¨ì„±" else "woman" if gender == "ì—¬ì„±" else "person"
        
        # ìƒ‰ìƒ/íƒ€ì… ë³€í™˜
        color_map = {
            "ê²€ì€ìƒ‰": "black", "í°ìƒ‰": "white", "ë¹¨ê°„ìƒ‰": "red", "íŒŒë€ìƒ‰": "blue",
            "ë…¸ë€ìƒ‰": "yellow", "ì´ˆë¡ìƒ‰": "green", "ë¶„í™ìƒ‰": "pink", "ë³´ë¼ìƒ‰": "purple",
            "íšŒìƒ‰": "gray", "ê°ˆìƒ‰": "brown", "ë² ì´ì§€": "beige", "ì¹´í‚¤": "khaki",
            "ë„¤ì´ë¹„": "navy", "ì˜¤ë Œì§€": "orange", "íŒŒìŠ¤í…”": "pastel"
        }
        
        # ì•„ì´í…œì„ ìµœëŒ€ 2ê°œë¡œ ì œí•œ (ì •í™•ë„ í–¥ìƒ)
        processed_items = []
        for item in items[:2]:  # 3ê°œ -> 2ê°œë¡œ ì œí•œ
            # ë¸Œëœë“œëª… ì œê±°
            for brand in ["ìœ ë‹ˆí´ë¡œ", "ë¦¬ë°”ì´ìŠ¤", "ì»¨ë²„ìŠ¤", "ë‚˜ì´í‚¤", "ì•„ë””ë‹¤ìŠ¤", "ìë¼", "H&M", 
                         "uniqlo", "levis", "converse", "nike", "adidas", "zara"]:
                item = item.replace(brand, "").strip()
            
            # ìƒ‰ìƒ ì˜ì–´ ë³€í™˜
            for kr_color, en_color in color_map.items():
                if kr_color in item:
                    item = item.replace(kr_color, en_color)
            
            # íƒ€ì… ì˜ì–´ ë³€í™˜
            item = item.replace("ë°˜íŒ”", "short sleeve").replace("ê¸´íŒ”", "long sleeve")
            item = item.replace("í‹°ì…”ì¸ ", "t-shirt").replace("ì…”ì¸ ", "shirt")
            item = item.replace("ë°”ì§€", "pants").replace("ë°˜ë°”ì§€", "shorts")
            item = item.replace("ì¬í‚·", "jacket").replace("ê°€ë””ê±´", "cardigan")
            item = item.replace("ë¶€ì¸ ", "boots").replace("ìŠ¤ë‹ˆì»¤ì¦ˆ", "sneakers")
            item = " ".join(item.split())
            
            if item:
                processed_items.append(item)
        
        # ì•„ì´í…œì„ ê°„ê²°í•˜ê²Œ í‘œí˜„
        if len(processed_items) >= 2:
            items_text = f"{processed_items[0]}, {processed_items[1]}"
        elif len(processed_items) == 1:
            items_text = processed_items[0]
        else:
            items_text = "casual outfit"
        
        # ìƒ‰ìƒ ì •í™•ë„ ìµœìš°ì„  í”„ë¡¬í”„íŠ¸ (UPPERCASE ê°•ì¡°)
        if len(processed_items) >= 2:
            # ê° ì•„ì´í…œì˜ ìƒ‰ìƒì„ UPPERCASEë¡œ ê°•ì¡°
            item1_upper = processed_items[0].upper()
            item2_upper = processed_items[1].upper()
            prompt = (
                f"one single mannequin only, {item1_upper}, "
                f"{item2_upper}, EXACT COLORS, product photo, centered"
            )
        elif len(processed_items) == 1:
            item_upper = processed_items[0].upper()
            prompt = f"single mannequin wearing {item_upper}, EXACT COLOR, product photo"
        else:
            prompt = "single mannequin with clothing"
        
        return prompt
    
    def generate_outfit_image(self, outfit_description: Dict, style_info: Dict = None) -> Optional[Image.Image]:
        """ì½”ë”” ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ AI ì´ë¯¸ì§€ ìƒì„±"""
        try:
            self._load_pipeline()
            
            prompt = self._build_prompt(outfit_description)
            # ë§¤ìš° ê°•ë ¥í•œ negative prompt (ì–¼êµ´ ì œê±° + ì—¬ëŸ¬ ë§ˆë„¤í‚¹ ì œê±° + ìƒ‰ìƒ í˜¼ë™ ë°©ì§€)
            negative_prompt = (
                "face, head, eyes, nose, mouth, lips, hair, neck, portrait, person, human face, "
                "multiple people, multiple mannequins, two mannequins, crowd, group, "
                "wrong colors, incorrect colors, color swap, reversed colors, "
                "white pants, shorts, blurry, low quality"
            )
            
            print(f"ì´ë¯¸ì§€ ìƒì„± ì¤‘... í”„ë¡¬í”„íŠ¸: {prompt[:80]}...")
            print(f"â³ ìƒì„± ì‹œê°„: ì•½ 15-30ì´ˆ")
            
            # íŒ¨ì¹˜: prepare_latentsë¥¼ MPSì—ì„œ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •
            import types
            original_prepare_latents = self.pipe.prepare_latents
            
            def patched_prepare_latents(self_pipe, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):
                if latents is None:
                    latents = original_prepare_latents(
                        batch_size, num_channels_latents, height, width, dtype, self.device, generator, None
                    )
                    # ì´ë¯¸ MPSë¡œ ìƒì„±ë˜ì§€ë§Œ í™•ì¸
                    if isinstance(latents, torch.Tensor) and latents.device.type != self.device:
                        latents = latents.to(self.device, non_blocking=False)
                else:
                    # latentsê°€ ì œê³µëœ ê²½ìš°ì—ë„ MPSë¡œ ì´ë™
                    if isinstance(latents, torch.Tensor) and latents.device.type != self.device:
                        latents = latents.to(self.device, non_blocking=False)
                return latents
            
            # UNet forward íŒ¨ì¹˜: encoder_hidden_statesë¥¼ MPSë¡œ ê°•ì œ ì´ë™
            original_unet_forward = self.pipe.unet.forward
            
            def patched_unet_forward(self_unet, sample, timestep, encoder_hidden_states=None, timestep_cond=None, **kwargs):
                # ëª¨ë“  ì…ë ¥ í…ì„œë¥¼ MPSë¡œ ì´ë™
                if isinstance(sample, torch.Tensor) and sample.device.type != self.device:
                    sample = sample.to(self.device, non_blocking=False)
                
                if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                    timestep = timestep.to(self.device, non_blocking=False)
                elif not isinstance(timestep, torch.Tensor):
                    timestep = torch.tensor([timestep], device=self.device, dtype=torch.long)
                
                # encoder_hidden_statesëŠ” ë°˜ë“œì‹œ MPSë¡œ ì´ë™
                if encoder_hidden_states is not None:
                    if isinstance(encoder_hidden_states, torch.Tensor):
                        if encoder_hidden_states.device.type != self.device:
                            encoder_hidden_states = encoder_hidden_states.to(self.device, non_blocking=False)
                    elif isinstance(encoder_hidden_states, (list, tuple)):
                        encoder_hidden_states = tuple(
                            h.to(self.device, non_blocking=False) if isinstance(h, torch.Tensor) and h.device.type != self.device else h 
                            for h in encoder_hidden_states
                        )
                
                if timestep_cond is not None and isinstance(timestep_cond, torch.Tensor) and timestep_cond.device.type != self.device:
                    timestep_cond = timestep_cond.to(self.device, non_blocking=False)
                
                # kwargsì˜ í…ì„œë„ MPSë¡œ ì´ë™
                for key, value in kwargs.items():
                    if isinstance(value, torch.Tensor) and value.device.type != self.device:
                        kwargs[key] = value.to(self.device, non_blocking=False)
                    elif isinstance(value, (list, tuple)):
                        kwargs[key] = type(value)(
                            v.to(self.device, non_blocking=False) if isinstance(v, torch.Tensor) and v.device.type != self.device else v 
                            for v in value
                        )
                
                return original_unet_forward(sample, timestep, encoder_hidden_states, timestep_cond, **kwargs)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ step íŒ¨ì¹˜: ëª¨ë“  í…ì„œë¥¼ MPSë¡œ ì´ë™
            original_scheduler_step = self.pipe.scheduler.step
            
            def patched_scheduler_step(self_scheduler, model_output, timestep, sample, return_dict=True, **kwargs):
                # ëª¨ë“  ì…ë ¥ í…ì„œë¥¼ MPSë¡œ ì´ë™
                if isinstance(model_output, torch.Tensor) and model_output.device.type != self.device:
                    model_output = model_output.to(self.device, non_blocking=False)
                if isinstance(sample, torch.Tensor) and sample.device.type != self.device:
                    sample = sample.to(self.device, non_blocking=False)
                if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                    timestep = timestep.to(self.device, non_blocking=False)
                elif not isinstance(timestep, torch.Tensor):
                    timestep = torch.tensor([timestep], device=self.device, dtype=torch.long)
                
                result = original_scheduler_step(model_output, timestep, sample, return_dict=return_dict, **kwargs)
                
                # ê²°ê³¼ í…ì„œë„ MPSë¡œ ì´ë™
                if isinstance(result, tuple):
                    result = tuple(
                        r.to(self.device, non_blocking=False) if isinstance(r, torch.Tensor) and r.device.type != self.device else r
                        for r in result
                    )
                elif isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, torch.Tensor) and value.device.type != self.device:
                            result[key] = value.to(self.device, non_blocking=False)
                
                return result
            
            # VAE decode íŒ¨ì¹˜: latentsë¥¼ CPUë¡œ ì´ë™ (VAEëŠ” CPUì—ì„œ ì‹¤í–‰)
            original_vae_decode = self.pipe.vae.decode
            
            def patched_vae_decode(self_vae, z, return_dict=True, **kwargs):
                # z(latents)ê°€ MPSì— ìˆìœ¼ë©´ CPUë¡œ ì´ë™
                if isinstance(z, torch.Tensor) and z.device.type == "mps":
                    z = z.to("cpu", non_blocking=False)
                return original_vae_decode(z, return_dict=return_dict, **kwargs)
            
            # íŒ¨ì¹˜ ì ìš©
            self.pipe.prepare_latents = types.MethodType(patched_prepare_latents, self.pipe)
            self.pipe.unet.forward = types.MethodType(patched_unet_forward, self.pipe.unet)
            self.pipe.scheduler.step = types.MethodType(patched_scheduler_step, self.pipe.scheduler)
            self.pipe.vae.decode = types.MethodType(patched_vae_decode, self.pipe.vae)
            
            try:
                with torch.no_grad():
                    # encode_promptë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì—¬ prompt_embeds ìƒì„±
                    prompt_embeds, negative_prompt_embeds = self.pipe.encode_prompt(
                        prompt=prompt,
                        device=torch.device("cpu"),  # TextEncoderëŠ” CPU
                        num_images_per_prompt=1,
                        do_classifier_free_guidance=True,
                        negative_prompt=negative_prompt
                    )
                    
                    # prompt_embedsë¥¼ MPSë¡œ ì´ë™
                    if isinstance(prompt_embeds, torch.Tensor):
                        prompt_embeds = prompt_embeds.to(self.device, non_blocking=False)
                    if isinstance(negative_prompt_embeds, torch.Tensor):
                        negative_prompt_embeds = negative_prompt_embeds.to(self.device, non_blocking=False)
                    
                    # pipe() í˜¸ì¶œ ì‹œ prompt_embeds ì‚¬ìš©
                    # SDXL-TurboëŠ” 1-4 steps, guidance_scale 0.0 ê¶Œì¥
                    # SD 1.4ëŠ” ê¸°ì¡´ ì„¤ì • ìœ ì§€
                    seed = 42
                    generator_obj = torch.Generator(device="cpu").manual_seed(seed)
                    
                    # SDXL-Turbo ê°ì§€ (ëª¨ë¸ëª…ìœ¼ë¡œ íŒë‹¨)
                    is_sdxl_turbo = "sdxl-turbo" in str(self.pipe.config._name_or_path).lower()
                    
                    if is_sdxl_turbo:
                        # SDXL-Turbo: 1 step, no guidance (ë¹ ë¥´ê³  ì •í™•)
                        result = self.pipe(
                            prompt_embeds=prompt_embeds,
                            negative_prompt_embeds=negative_prompt_embeds,
                            num_inference_steps=4,  # TurboëŠ” 1-4 steps
                            guidance_scale=0.0,  # TurboëŠ” guidance ë¶ˆí•„ìš”
                            height=512,
                            width=512,
                            generator=generator_obj
                        )
                    else:
                        # SD 1.4: ê¸°ì¡´ ì„¤ì •
                        result = self.pipe(
                            prompt_embeds=prompt_embeds,
                            negative_prompt_embeds=negative_prompt_embeds,
                            num_inference_steps=30,
                            guidance_scale=15.0,
                            height=512,
                            width=512,
                            generator=generator_obj
                        )
                
                image = result.images[0]
                
                # í›„ì²˜ë¦¬: ìƒë‹¨ 40% í¬ë¡­í•˜ì—¬ ì–¼êµ´/ëª© ì™„ì „ ì œê±°
                width, height = image.size
                crop_top = int(height * 0.40)  # ìƒë‹¨ 40% ì œê±° (ëª©ê¹Œì§€ ì œê±°)
                cropped_image = image.crop((0, crop_top, width, height))
                
                # ì›ë˜ í¬ê¸°ë¡œ ì¡°ì • (í°ìƒ‰ ì—¬ë°± ì¶”ê°€)
                from PIL import Image as PILImage
                final_image = PILImage.new('RGB', (width, height), color=(255, 255, 255))
                final_image.paste(cropped_image, (0, 0))
                
                print("âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ (ì–¼êµ´/ëª© ì™„ì „ ì œê±°)")
                return final_image
            finally:
                # íŒ¨ì¹˜ ë³µì›
                self.pipe.prepare_latents = original_prepare_latents
                self.pipe.unet.forward = original_unet_forward
                self.pipe.scheduler.step = original_scheduler_step
                self.pipe.vae.decode = original_vae_decode
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
