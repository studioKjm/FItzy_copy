"""
ì¶”ì²œ ì½”ë”” AI ì´ë¯¸ì§€ ìƒì„± ìœ í‹¸ë¦¬í‹°
Stable Diffusion ë¡œì»¬ ì‹¤í–‰ (MPS ìµœì í™”)
"""

import os
from PIL import Image
from typing import Optional, Dict
import torch
from diffusers import StableDiffusionPipeline


class OutfitImageGenerator:
    """ì¶”ì²œ ì½”ë”” AI ì´ë¯¸ì§€ ìƒì„± í´ë˜ìŠ¤ - ê°„ì†Œí™” ë²„ì „"""
    
    def __init__(self, method: str = "stable_diffusion"):
        """
        Args:
            method: ì´ë¯¸ì§€ ìƒì„± ë°©ë²• (í˜„ì¬ëŠ” "stable_diffusion"ë§Œ ì§€ì›)
        """
        self.method = method
        
        # MPS ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not (hasattr(torch.backends, 'mps') and torch.backends.mps.is_available()):
            raise RuntimeError("âŒ MPS (GPU)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. MacBook M1/M2ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        # íŒŒì´í”„ë¼ì¸ì€ ë‚˜ì¤‘ì— ë¡œë“œ (ì§€ì—° ë¡œë”©)
        self.pipe = None
        self.device = "mps"
        self.vae_device = "cpu"
    
    def _load_pipeline(self):
        """íŒŒì´í”„ë¼ì¸ ì§€ì—° ë¡œë”©"""
        if self.pipe is not None:
            return
        
        print("ğŸ Apple Silicon (M1/M2) ê°ì§€ - MPS ë°±ì—”ë“œ ì‚¬ìš© (GPU ê°€ì†)")
        print("Stable Diffusion ëª¨ë¸ ë¡œë“œ ì¤‘... (ì¥ì¹˜: mps, ëª¨ë¸: CompVis/stable-diffusion-v1-4)")
        print("â³ ì²˜ìŒ ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œê°€ í•„ìš”í•©ë‹ˆë‹¤ (ì•½ 4GB, ëª‡ ë¶„ ì†Œìš”)")
        
        # ëª¨ë¸ ë¡œë“œ
        self.pipe = StableDiffusionPipeline.from_pretrained(
            "CompVis/stable-diffusion-v1-4",
            torch_dtype=torch.float32,
            safety_checker=None,
            requires_safety_checker=False,
            device_map=None
        )
        
        # ì»´í¬ë„ŒíŠ¸ë³„ ì¥ì¹˜ ë°°ì¹˜
        self.pipe.vae = self.pipe.vae.to(self.vae_device, non_blocking=False)
        self.pipe.unet = self.pipe.unet.float().to(self.device, non_blocking=False)
        self.pipe.text_encoder = self.pipe.text_encoder.float().to("cpu", non_blocking=False)
        
        # ìµœì í™” ì„¤ì •
        self.pipe.enable_attention_slicing()
        try:
            self.pipe.enable_xformers_memory_efficient_attention()
        except:
            pass
        
        torch.mps.synchronize()
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    def _build_prompt(self, outfit_description: Dict) -> str:
        """íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        items = outfit_description.get("items", [])
        style = outfit_description.get("style", "ìºì£¼ì–¼")
        gender = outfit_description.get("gender", "ê³µìš©")
        
        # ì„±ë³„ í‚¤ì›Œë“œ
        gender_keyword = "male model, man" if gender == "ë‚¨ì„±" else \
                        "female model, woman" if gender == "ì—¬ì„±" else "model"
        
        # ìƒ‰ìƒ/íƒ€ì… ë³€í™˜
        color_map = {
            "ê²€ì€ìƒ‰": "black", "í°ìƒ‰": "white", "ë¹¨ê°„ìƒ‰": "red", "íŒŒë€ìƒ‰": "blue",
            "ë…¸ë€ìƒ‰": "yellow", "ì´ˆë¡ìƒ‰": "green", "ë¶„í™ìƒ‰": "pink", "ë³´ë¼ìƒ‰": "purple",
            "íšŒìƒ‰": "gray", "ê°ˆìƒ‰": "brown", "ë² ì´ì§€": "beige", "ì¹´í‚¤": "khaki",
            "ë„¤ì´ë¹„": "navy", "ì˜¤ë Œì§€": "orange", "íŒŒìŠ¤í…”": "pastel"
        }
        
        processed_items = []
        for item in items[:3]:
            # ë¸Œëœë“œëª… ì œê±°
            for brand in ["ìœ ë‹ˆí´ë¡œ", "ë¦¬ë°”ì´ìŠ¤", "ì»¨ë²„ìŠ¤", "ë‚˜ì´í‚¤", "ì•„ë””ë‹¤ìŠ¤", "ìë¼", "H&M", 
                         "uniqlo", "levis", "converse", "nike", "adidas", "zara", "u ", "U "]:
                item = item.replace(brand, "").strip()
            
            # ìƒ‰ìƒ ì˜ì–´ ë³€í™˜
            for kr_color, en_color in color_map.items():
                if kr_color in item:
                    item = item.replace(kr_color, en_color)
            
            # íƒ€ì… ì˜ì–´ ë³€í™˜
            item = item.replace("ë°˜íŒ”", "short sleeve").replace("ê¸´íŒ”", "long sleeve")
            item = item.replace("í‹°ì…”ì¸ ", "t-shirt").replace("ì…”ì¸ ", "shirt")
            item = item.replace("ë°”ì§€", "long pants").replace("ë°˜ë°”ì§€", "shorts")
            item = item.replace("ì¬í‚·", "jacket").replace("ê°€ë””ê±´", "cardigan")
            item = item.replace("ë¶€ì¸ ", "boots").replace("ìŠ¤ë‹ˆì»¤ì¦ˆ", "sneakers")
            item = " ".join(item.split())
            
            if item:
                processed_items.append(item)
        
        items_text = ", ".join(processed_items) if processed_items else "fashion outfit"
        
        # ìŠ¤íƒ€ì¼ í‚¤ì›Œë“œ
        style_keywords = {
            "ìºì£¼ì–¼": "casual style",
            "í¬ë©€": "formal elegant style",
            "íŠ¸ë Œë””": "trendy modern style"
        }
        style_en = style_keywords.get(style, "casual style")
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (CLIP 77 í† í° ì œí•œ ì¤€ìˆ˜, í•µì‹¬ë§Œ)
        prompt = (
            f"headless mannequin, no face, neck to feet, "
            f"{gender_keyword} wearing {items_text}, "
            f"{style_en}, exact colors, full body, white background"
        )
        
        return prompt
    
    def generate_outfit_image(self, outfit_description: Dict, style_info: Dict = None) -> Optional[Image.Image]:
        """ì½”ë”” ì„¤ëª…ì„ ë°”íƒ•ìœ¼ë¡œ AI ì´ë¯¸ì§€ ìƒì„±"""
        try:
            self._load_pipeline()
            
            prompt = self._build_prompt(outfit_description)
            negative_prompt = (
                "face, head, portrait, hair, wrong colors, color mismatch, "
                "blurry, low quality, deformed, cropped body, text"
            )
            
            print(f"ì´ë¯¸ì§€ ìƒì„± ì¤‘... í”„ë¡¬í”„íŠ¸: {prompt[:80]}...")
            print(f"â³ ìƒì„± ì‹œê°„: ì•½ 15-30ì´ˆ")
            
            # íŒ¨ì¹˜: prepare_latentsë¥¼ MPSì—ì„œ ìƒì„±í•˜ë„ë¡ ìˆ˜ì •
            import types
            original_prepare_latents = self.pipe.prepare_latents
            
            def patched_prepare_latents(self_pipe, batch_size, num_channels_latents, height, width, dtype, device, generator, latents=None):
                if latents is None:
                    latents = original_prepare_latents(
                        batch_size, num_channels_latents, height, width, dtype, self.device, generator, None
                    )
                    # ì´ë¯¸ MPSë¡œ ìƒì„±ë˜ì§€ë§Œ í™•ì¸
                    if isinstance(latents, torch.Tensor) and latents.device.type != self.device:
                        latents = latents.to(self.device, non_blocking=False)
                else:
                    # latentsê°€ ì œê³µëœ ê²½ìš°ì—ë„ MPSë¡œ ì´ë™
                    if isinstance(latents, torch.Tensor) and latents.device.type != self.device:
                        latents = latents.to(self.device, non_blocking=False)
                return latents
            
            # UNet forward íŒ¨ì¹˜: encoder_hidden_statesë¥¼ MPSë¡œ ê°•ì œ ì´ë™
            original_unet_forward = self.pipe.unet.forward
            
            def patched_unet_forward(self_unet, sample, timestep, encoder_hidden_states=None, timestep_cond=None, **kwargs):
                # ëª¨ë“  ì…ë ¥ í…ì„œë¥¼ MPSë¡œ ì´ë™
                if isinstance(sample, torch.Tensor) and sample.device.type != self.device:
                    sample = sample.to(self.device, non_blocking=False)
                
                if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                    timestep = timestep.to(self.device, non_blocking=False)
                elif not isinstance(timestep, torch.Tensor):
                    timestep = torch.tensor([timestep], device=self.device, dtype=torch.long)
                
                # encoder_hidden_statesëŠ” ë°˜ë“œì‹œ MPSë¡œ ì´ë™
                if encoder_hidden_states is not None:
                    if isinstance(encoder_hidden_states, torch.Tensor):
                        if encoder_hidden_states.device.type != self.device:
                            encoder_hidden_states = encoder_hidden_states.to(self.device, non_blocking=False)
                    elif isinstance(encoder_hidden_states, (list, tuple)):
                        encoder_hidden_states = tuple(
                            h.to(self.device, non_blocking=False) if isinstance(h, torch.Tensor) and h.device.type != self.device else h 
                            for h in encoder_hidden_states
                        )
                
                if timestep_cond is not None and isinstance(timestep_cond, torch.Tensor) and timestep_cond.device.type != self.device:
                    timestep_cond = timestep_cond.to(self.device, non_blocking=False)
                
                # kwargsì˜ í…ì„œë„ MPSë¡œ ì´ë™
                for key, value in kwargs.items():
                    if isinstance(value, torch.Tensor) and value.device.type != self.device:
                        kwargs[key] = value.to(self.device, non_blocking=False)
                    elif isinstance(value, (list, tuple)):
                        kwargs[key] = type(value)(
                            v.to(self.device, non_blocking=False) if isinstance(v, torch.Tensor) and v.device.type != self.device else v 
                            for v in value
                        )
                
                return original_unet_forward(sample, timestep, encoder_hidden_states, timestep_cond, **kwargs)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ step íŒ¨ì¹˜: ëª¨ë“  í…ì„œë¥¼ MPSë¡œ ì´ë™
            original_scheduler_step = self.pipe.scheduler.step
            
            def patched_scheduler_step(self_scheduler, model_output, timestep, sample, return_dict=True, **kwargs):
                # ëª¨ë“  ì…ë ¥ í…ì„œë¥¼ MPSë¡œ ì´ë™
                if isinstance(model_output, torch.Tensor) and model_output.device.type != self.device:
                    model_output = model_output.to(self.device, non_blocking=False)
                if isinstance(sample, torch.Tensor) and sample.device.type != self.device:
                    sample = sample.to(self.device, non_blocking=False)
                if isinstance(timestep, torch.Tensor) and timestep.device.type != self.device:
                    timestep = timestep.to(self.device, non_blocking=False)
                elif not isinstance(timestep, torch.Tensor):
                    timestep = torch.tensor([timestep], device=self.device, dtype=torch.long)
                
                result = original_scheduler_step(model_output, timestep, sample, return_dict=return_dict, **kwargs)
                
                # ê²°ê³¼ í…ì„œë„ MPSë¡œ ì´ë™
                if isinstance(result, tuple):
                    result = tuple(
                        r.to(self.device, non_blocking=False) if isinstance(r, torch.Tensor) and r.device.type != self.device else r
                        for r in result
                    )
                elif isinstance(result, dict):
                    for key, value in result.items():
                        if isinstance(value, torch.Tensor) and value.device.type != self.device:
                            result[key] = value.to(self.device, non_blocking=False)
                
                return result
            
            # VAE decode íŒ¨ì¹˜: latentsë¥¼ CPUë¡œ ì´ë™ (VAEëŠ” CPUì—ì„œ ì‹¤í–‰)
            original_vae_decode = self.pipe.vae.decode
            
            def patched_vae_decode(self_vae, z, return_dict=True, **kwargs):
                # z(latents)ê°€ MPSì— ìˆìœ¼ë©´ CPUë¡œ ì´ë™
                if isinstance(z, torch.Tensor) and z.device.type == "mps":
                    z = z.to("cpu", non_blocking=False)
                return original_vae_decode(z, return_dict=return_dict, **kwargs)
            
            # íŒ¨ì¹˜ ì ìš©
            self.pipe.prepare_latents = types.MethodType(patched_prepare_latents, self.pipe)
            self.pipe.unet.forward = types.MethodType(patched_unet_forward, self.pipe.unet)
            self.pipe.scheduler.step = types.MethodType(patched_scheduler_step, self.pipe.scheduler)
            self.pipe.vae.decode = types.MethodType(patched_vae_decode, self.pipe.vae)
            
            try:
                with torch.no_grad():
                    # encode_promptë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì—¬ prompt_embeds ìƒì„±
                    prompt_embeds, negative_prompt_embeds = self.pipe.encode_prompt(
                        prompt=prompt,
                        device=torch.device("cpu"),  # TextEncoderëŠ” CPU
                        num_images_per_prompt=1,
                        do_classifier_free_guidance=True,
                        negative_prompt=negative_prompt
                    )
                    
                    # prompt_embedsë¥¼ MPSë¡œ ì´ë™
                    if isinstance(prompt_embeds, torch.Tensor):
                        prompt_embeds = prompt_embeds.to(self.device, non_blocking=False)
                    if isinstance(negative_prompt_embeds, torch.Tensor):
                        negative_prompt_embeds = negative_prompt_embeds.to(self.device, non_blocking=False)
                    
                    # pipe() í˜¸ì¶œ ì‹œ prompt_embeds ì‚¬ìš©
                    # guidance_scale ë†’ì—¬ì„œ í”„ë¡¬í”„íŠ¸ ì •í™•ë„ í–¥ìƒ (7.5 -> 9.5)
                    result = self.pipe(
                        prompt_embeds=prompt_embeds,
                        negative_prompt_embeds=negative_prompt_embeds,
                        num_inference_steps=20,  # 15 -> 20 (ì •í™•ë„ í–¥ìƒ)
                        guidance_scale=9.5,  # í”„ë¡¬í”„íŠ¸ ì¤€ìˆ˜ë„ í–¥ìƒ
                        height=512,
                        width=512,
                        generator=None
                    )
                
                image = result.images[0]
                print("âœ… ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ!")
                return image
            finally:
                # íŒ¨ì¹˜ ë³µì›
                self.pipe.prepare_latents = original_prepare_latents
                self.pipe.unet.forward = original_unet_forward
                self.pipe.scheduler.step = original_scheduler_step
                self.pipe.vae.decode = original_vae_decode
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
